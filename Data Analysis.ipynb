{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents\n",
    "import json\n",
    "with open('jsonformatter.txt', 'r') as f:\n",
    "    with open(\"Precedents output.txt\", 'w') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # print(type(data['label']))\n",
    "        for case in data:\n",
    "            out.write('Case id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            # print(annotation['result'])\n",
    "            for key in annotation['result']:\n",
    "                # out.write(key['value']['labels'][0] + '\\n')\n",
    "                if key['value']['labels'][0] == 'STA':\n",
    "                    out.write('STA : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents relied\n",
    "import json\n",
    "with open('jsonformatter.txt', 'r') as f:\n",
    "    with open(\"Precedents relied output.txt\", 'w') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'PRE_RELIED':\n",
    "                    out.write('PRE_RELIED : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents not relied\n",
    "import json\n",
    "with open('jsonformatter.txt', 'r') as f:\n",
    "    with open(\"Precedents not relied output.txt\", 'w') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'PRE_NOT_RELIED':\n",
    "                    out.write('PRE_NOT_RELIED : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42899f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To seperate the precedents on the keyword \"vs\" or \"v.\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        # out.write(\"Inside Judgement Text : \" + legis.tag + '\\n')\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    out.write(\"Precedent : \" + para.text + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                out.write(\"Precedent : \" + legis.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To classify precedents\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents classified output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    if 'overruled' in s1:\n",
    "                                        out.write(\"Precedent Overruled : \")\n",
    "                                    elif 'distinguish' in s1:\n",
    "                                        out.write(\"Precedent Distinguished : \")\n",
    "                                    elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                        out.write(\"Precedent Relied : \")\n",
    "                                    else:\n",
    "                                        out.write(\"Precedent Referred : \")\n",
    "                                    out.write(s2 + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                if 'overruled' in s1:\n",
    "                                    out.write(\"Precedent Overruled : \")\n",
    "                                elif 'distinguish' in s1:\n",
    "                                    out.write(\"Precedent Distinguished : \")\n",
    "                                elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                    out.write(\"Precedent Relied : \")\n",
    "                                else:\n",
    "                                    out.write(\"Precedent Referred : \")\n",
    "                                out.write(s2 + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents classified output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                flag = 0\n",
    "                                s = \"\"\n",
    "                                for sent in sent_tokenize(para.text):\n",
    "                                    s1 = sent.lower()\n",
    "                                    s2 = sent\n",
    "                                    if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                        flag=1\n",
    "                                        s += s2\n",
    "                                if flag:\n",
    "                                    if 'overruled' in s.lower():\n",
    "                                        out.write(\"Precedent Overruled : \")\n",
    "                                    elif 'distinguish' in s.lower():\n",
    "                                        out.write(\"Precedent Distinguished : \")\n",
    "                                    elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                        out.write(\"Precedent Relied : \")\n",
    "                                    else:\n",
    "                                        out.write(\"Precedent Referred : \")\n",
    "                                    out.write(s + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            flag = 0\n",
    "                            s = \"\"\n",
    "                            for sent in sent_tokenize(legis.text):\n",
    "                                s1 = sent.lower()\n",
    "                                s2 = sent\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    flag=1\n",
    "                                    s += s2\n",
    "                            if flag:\n",
    "                                if 'overruled' in s.lower():\n",
    "                                    out.write(\"Precedent Overruled : \")\n",
    "                                elif 'distinguish' in s.lower():\n",
    "                                    out.write(\"Precedent Distinguished : \")\n",
    "                                elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                    out.write(\"Precedent Relied : \")\n",
    "                                else:\n",
    "                                    out.write(\"Precedent Referred : \")\n",
    "                                out.write(s + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c078b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To differentiate between capital V and small v\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "with open( 'Precedents output.txt', 'w') as out:\n",
    "    directories = ['Writ Petition', 'Civil Appeal']\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            f = os.path.join(directory, filename)\n",
    "            mytree = ET.ElementTree(file= f)\n",
    "            myroot = mytree.getroot()\n",
    "\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        # out.write(\"Inside Judgement Text : \" + legis.tag + '\\n')\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if 'overrule' in s1: # or (' v ' in s1 and ' v ' not in s2): \n",
    "                                    out.write(directory + ' : ' + filename  + ' : ' + \"Precedent : \" + para.text + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            \n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if 'overrule' in s1: # or (' v ' in s1 and ' v ' not in s2): \n",
    "                                out.write(directory + ' : '  + filename + ' : '  + \"Precedent : \" + legis.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7681067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents in pdf format\n",
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "# import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "directories = ['New cases']\n",
    "for directory in directories:\n",
    "    out_dir = directory+' precedents classified output'\n",
    "    for filename in os.listdir(directory):\n",
    "        # f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".pdf\", \" output.txt\"))\n",
    "        out_file.encode('utf-8')\n",
    "        with open(\"{}/{}\".format(directory,filename), 'r', encoding = 'utf-8') as pdfFileObj:\n",
    "            object = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "            NumPages = object.getNumPages()\n",
    "            with open( out_file, 'w') as out:\n",
    "                for i in range(NumPages):\n",
    "                    Text = object.getPage(i).extractText()\n",
    "                    flag = 0\n",
    "                    s = \"\"\n",
    "                    for sent in sent_tokenize(Text):\n",
    "                        s1 = sent.lower()\n",
    "                        s2 = sent\n",
    "                        if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                            flag=1\n",
    "                            s += s2\n",
    "                    if flag:\n",
    "                        if 'overruled' in s.lower():\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s.lower():\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(s + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f20bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "# import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.xml' in filename:\n",
    "            f = os.path.join(directory, filename)\n",
    "            mytree = ET.parse(f)\n",
    "            # myroot = mytree.getroot()\n",
    "\n",
    "            out_dir = directory+' precedents classified output'\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            with open( out_file, 'w') as out:\n",
    "                for i in mytree.iter():\n",
    "                    if i.text is not None:\n",
    "                        out.write(i.text + '\\n')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents from txt file for 26 cases\n",
    "\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['Twenty_six']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_dir = directory+' output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".txt\", \" output.txt\"))\n",
    "        with open(f, 'r') as in_file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                for s2 in in_file:\n",
    "                    # out.write(line + '\\n')\n",
    "                    flag = 0\n",
    "                    s1 = s2.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                        flag=1\n",
    "                    if flag:\n",
    "                        if 'overruled' in s1.lower():\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s1.lower():\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(s1 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract and classify precedents from any XML file with text within JudgementText\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    out_dir = directory+' output'\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.xml' in filename:\n",
    "            f = os.path.join(directory, filename)\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            with open(f, 'r') as file:\n",
    "                with open( out_file, 'w') as out:\n",
    "                    plain_text = \"\"\n",
    "                    for line in file:\n",
    "                        plain_text += line\n",
    "                    sub1 = '<JudgmentText>'\n",
    "                    sub2 = '</JudgmentText>'\n",
    "                    idx1 = plain_text.index(sub1) + len(sub1)\n",
    "                    idx2 = plain_text.index(sub2)\n",
    "                    text = \"\"\n",
    "                    flag=1\n",
    "                    for i in range(idx1, idx2):\n",
    "                        char = plain_text[i]\n",
    "                        if char == '<':\n",
    "                            flag=0\n",
    "                        elif char == '>':\n",
    "                            flag=1\n",
    "                        elif flag:\n",
    "                            # if char != '\\n':# or text[-1] == '.':\n",
    "                            text += char\n",
    "                    text = \" \".join(text.split())\n",
    "                    # out.write(text + '\\n\\n')\n",
    "                    sentences = sent_tokenize(text)\n",
    "                    flag=0\n",
    "                    prnt_snt = \"\"\n",
    "                    for sent in sentences:\n",
    "                        # out.write(sent + '\\n\\n')\n",
    "                        s1 = sent.lower()\n",
    "                        if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                            flag = 1\n",
    "                            prnt_snt = sent\n",
    "                        elif flag:\n",
    "                            prnt_snt += sent\n",
    "                            s1 = prnt_snt.lower()\n",
    "                            if 'overruled' in s1:\n",
    "                                out.write(\"Precedent Overruled : \")\n",
    "                            elif 'distinguish' in s1:\n",
    "                                out.write(\"Precedent Distinguished : \")\n",
    "                            elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                                out.write(\"Precedent Relied : \")\n",
    "                            else:\n",
    "                                out.write(\"Precedent Referred : \")\n",
    "                            out.write(prnt_snt + '\\n\\n')\n",
    "                            flag = 0\n",
    "                            prnt_snt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract and classify precedents from any XML file with text within JudgementText\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "# import en_core_web_sm\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    out_dir = directory+' output'\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.xml' in filename:\n",
    "            f = os.path.join(directory, filename)\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            with open(f, 'r') as file:\n",
    "                with open( out_file, 'w') as out:\n",
    "                    plain_text = \"\"\n",
    "                    for line in file:\n",
    "                        plain_text += line\n",
    "                    sub1 = '<JudgmentText>'\n",
    "                    sub2 = '</JudgmentText>'\n",
    "                    idx1 = plain_text.index(sub1) + len(sub1)\n",
    "                    idx2 = plain_text.index(sub2)\n",
    "                    text = \"\"\n",
    "                    flag=1\n",
    "                    for i in range(idx1, idx2):\n",
    "                        char = plain_text[i]\n",
    "                        if char == '<':\n",
    "                            flag=0\n",
    "                        elif char == '>':\n",
    "                            flag=1\n",
    "\n",
    "                        elif flag:\n",
    "                            # if char != '\\n':# or text[-1] == '.':\n",
    "                            text += char\n",
    "                    text = \" \".join(text.split())\n",
    "                    doc = nlp(text)\n",
    "                    # out.write(text + '\\n\\n')\n",
    "                    sentences = doc.sents\n",
    "                    flag=0\n",
    "                    prnt_snt = \"\"\n",
    "                    for sent in sentences:\n",
    "                        # out.write(sent + '\\n\\n')\n",
    "                        s1 = sent.lower()\n",
    "                        if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                            flag = 1\n",
    "                            prnt_snt = sent\n",
    "                        elif flag:\n",
    "                            prnt_snt += sent\n",
    "                            s1 = prnt_snt.lower()\n",
    "                            if 'overruled' in s1:\n",
    "                                out.write(\"Precedent Overruled : \")\n",
    "                            elif 'distinguish' in s1:\n",
    "                                out.write(\"Precedent Distinguished : \")\n",
    "                            elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                                out.write(\"Precedent Relied : \")\n",
    "                            else:\n",
    "                                out.write(\"Precedent Referred : \")\n",
    "                                \n",
    "                            out.write(prnt_snt + '\\n\\n')\n",
    "                            flag = 0\n",
    "                            prnt_snt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6e58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for the directory new_case_json_100\n",
    "import os\n",
    "import json\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['new_case_json_100']\n",
    "for directory in directories:\n",
    "    out_dir = directory+'_output'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".json\", \" output.txt\"))\n",
    "        with open(f, 'r') as file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                data = json.load(file)\n",
    "                text = \"\"\n",
    "                for line in data['JudgmentText']:\n",
    "                    if isinstance(line, str):\n",
    "                        text += line\n",
    "                    elif isinstance(line['I'], str):\n",
    "                        text += line['I']\n",
    "                        # print(filename, type(line['I']), line['I'])\n",
    "                    else:\n",
    "                        text += ''.join(line['I'])\n",
    "                # text = \" \".join(text.split())\n",
    "                # out.write(text + '\\n\\n')\n",
    "                sentences = sent_tokenize(text)\n",
    "                flag=0\n",
    "                prnt_snt = \"\"\n",
    "                for sent in sentences:\n",
    "                    # out.write(sent + '\\n\\n')\n",
    "                    s1 = sent.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                        flag = 1\n",
    "                        prnt_snt = sent\n",
    "                    elif flag:\n",
    "                        prnt_snt += sent\n",
    "                        s1 = prnt_snt.lower()\n",
    "                        if 'overruled' in s1:\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s1:\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(prnt_snt + '\\n\\n')\n",
    "                        flag = 0\n",
    "                        prnt_snt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for the directory new_case_json_100 with json output\n",
    "import os\n",
    "import json\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['new_case_json_100']\n",
    "for directory in directories:\n",
    "    out_dir = directory+'_output'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".json\", \" output.json\"))\n",
    "        with open(f, 'r') as file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                output = {}\n",
    "                data = json.load(file)\n",
    "                text = \"\"\n",
    "                for line in data['JudgmentText']:\n",
    "                    if isinstance(line, str):\n",
    "                        text += line\n",
    "                    elif isinstance(line['I'], str):\n",
    "                        text += line['I']\n",
    "                        # print(filename, type(line['I']), line['I'])\n",
    "                    else:\n",
    "                        text += ''.join(line['I'])\n",
    "                # text = \" \".join(text.split())\n",
    "                # out.write(text + '\\n\\n')\n",
    "                sentences = sent_tokenize(text)\n",
    "                flag=0\n",
    "                prnt_snt = \"\"\n",
    "                for sent in sentences:\n",
    "                    # out.write(sent + '\\n\\n')\n",
    "                    s1 = sent.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                        flag = 1\n",
    "                        prnt_snt = sent\n",
    "                    elif flag:\n",
    "                        prnt_snt += sent\n",
    "                        s1 = prnt_snt.lower()\n",
    "                        if 'overruled' in s1:\n",
    "                            # out.write(\"Precedent Overruled : \")\n",
    "                            output[prnt_snt] = \"PRE_OVER\"\n",
    "                        elif 'distinguish' in s1:\n",
    "                            # out.write(\"Precedent Distinguished : \")\n",
    "                            output[prnt_snt] = \"PRE_DIST\"\n",
    "                        elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                            # out.write(\"Precedent Relied : \")\n",
    "                            output[prnt_snt] = \"PRE_REL\"\n",
    "                        else:\n",
    "                            # out.write(\"Precedent Referred : \")\n",
    "                            output[prnt_snt] = \"PRE_REF\"\n",
    "                        # out.write(prnt_snt + '\\n\\n')\n",
    "                        flag = 0\n",
    "                        prnt_snt = \"\"\n",
    "\n",
    "                json_output = json.dumps(output, indent=4)\n",
    "                out.write(json_output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
